# 机器学习基础

2024/10/16

学习过程中chatgpt对话框链接https://chatgpt.com/share/670fd2d3-ac44-8000-baa9-bac839df82a1

## 安装PyTorch&cuda

这里需要先安装cuda再配，开始时弄错顺序了，最后环境越配越乱，难受了

总结：还是尽量用conda安装吧。。。而且网上太多教学太乱了（难受）

[cuda安装](【2024最新cuda与cudnn安装教程【深度学习环境配置 | 安装包】】https://www.bilibili.com/video/BV116eBefETi?vd_source=64fa735df4e10c3811ddac775f3035f1)

[pytorch配置](https://blog.csdn.net/weixin_53534399/article/details/125954715?ops_request_misc=%257B%2522request%255Fid%2522%253A%252286EF597A-804E-48CC-9AD1-2DA174143CAE%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=86EF597A-804E-48CC-9AD1-2DA174143CAE&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125954715-null-null.142^v100^pc_search_result_base4&utm_term=anaconda%E9%85%8D%E7%BD%AEpytorch%E7%8E%AF%E5%A2%83&spm=1018.2226.3001.4187)

<img src="https://raw.githubusercontent.com/Davereminisce/image/1f9d63436188d0a5b3e99f9747ed1da308cc7ddb/%7BD69328DA-1272-4FD8-8EE0-40B34443EF5B%7D.png" alt="img" style="zoom:50%;" />

#### **监督学习：（最广泛使用的机器学习形式）**

算法通过已标记的数据进行训练。训练数据集中的每个输入都有相应的输出，模型的任务是学习输入和输出之间的关系，以便在遇到新的、未标记的数据时能够准确预测输出。

**回归**（Regression）：预测连续的数值输出。例如，房价预测、股票价格预测等。<img src="https://github.com/Davereminisce/image/blob/master/%7B915304FA-68F0-4A21-98FF-EFA031EA6D18%7D.png?raw=true" alt="{915304FA-68F0-4A21-98FF-EFA031EA6D18}.png" style="zoom:67%;" />

**分类**（Classification）：将输入数据分类到预定义的类别中。例如，垃圾邮件过滤、图像分类等。

**<img src="https://github.com/Davereminisce/image/blob/master/%7B22E17223-5EF8-4979-B2EE-47839BF6FA22%7D.png?raw=true" alt="{22E17223-5EF8-4979-B2EE-47839BF6FA22}.png" style="zoom:67%;" />**

#### **无监督学习：**

无监督学习不依赖于带有标签的数据。它通过对数据本身的结构和模式进行分析和发现，以此来总结出有用的信息。无监督学习的目标是从未标记的数据中找到隐藏的模式、结构或关联。

**就是没有告诉数据输入和输出对应关系，自动学习一堆数据，在这堆数据中找到关系。**

**<img src="https://github.com/Davereminisce/image/blob/master/%7B03915205-7360-4F7A-8BB4-FE43EE990000%7D.png?raw=true" alt="{03915205-7360-4F7A-8BB4-FE43EE990000}.png" style="zoom: 67%;" />**

**聚类**（Clustering）：将数据划分为不同的组（簇），每个组中的数据在某种意义上相似。

**异常检测**：检测异常事件

**降维**（Dimensionality Reduction）：减少数据的特征数量，同时尽可能保留原始数据的信息。这有助于数据的可视化和处理大规模数据集。

**关联规则学习**（Association Rule Learning）：用于发现数据集中频繁出现的项集和它们之间的关联规则。

**自编码器**（Autoencoder）：一种神经网络，用于学习数据的简化表示（编码），然后再从简化表示中重建原始数据。它在数据压缩和特征提取中有应用。

#### 监督学习与无监督学习的区别

监督学习看作AB两个组AB间有关系，不断学习发现AB间映射关系。主要是预测

无监督学习只是找出A组内部的结构或关联或者特点。

|          | 监督学习           | 无监督学习               |
| -------- | ------------------ | ------------------------ |
| 数据类型 | 带标签的数据       | 未标记的数据             |
| 任务目标 | 学会从输入预测输出 | 发现数据的内在结构、模式 |

## overview

### 算法思维方式

算法：穷举法 贪心法 分治法 动态规划  

机器学习是机器通过数据集找出算法

**经典机器学习**![{C8D0C7B9-7EF4-416F-9F1F-C2E058315363}.png](https://github.com/Davereminisce/image/blob/master/%7BC8D0C7B9-7EF4-416F-9F1F-C2E058315363%7D.png?raw=true)

**表示机器学习**

Representation Learning（表示学习）是机器学习中的一个重要领域，它的目标是自动从原始数据中学习出有意义的特征表示。传统的机器学习模型通常需要依赖手工设计的特征，而表示学习试图通过模型来发现能够更好地描述数据的低维或高效的特征表示，从而提升模型的性能。

![{545EA697-1221-48F5-BF58-45635C5AD05E}.png](https://github.com/Davereminisce/image/blob/master/%7B545EA697-1221-48F5-BF58-45635C5AD05E%7D.png?raw=true)

降低至低维这样需要的数据就不需要那么多了

高维降低维

<img src="https://github.com/Davereminisce/image/blob/master/%7BE5954BE1-A750-493C-B5B2-AA2EDE38FF43%7D.png?raw=true" alt="{E5954BE1-A750-493C-B5B2-AA2EDE38FF43}.png" style="zoom:50%;" />

**深度学习（Deep Learning）**

属于表示学习的一个重要分支。深度学习可以被看作是一种高级的表示学习方法，通过深层神经网络自动学习数据的多层次表示，捕捉数据的复杂模式和特征。

是端到端的学习：指通过一个单一的模型从输入到输出直接进行学习和预测的过程。通过一个**统一的模型**，将原始数据直接输入到模型中，模型自动完成特征提取、特征变换和预测。

### **神经网络**

<img src="https://github.com/Davereminisce/image/blob/master/%7B380B8057-0B4B-4DD2-B139-BDECBE4A4D1B%7D.png?raw=true" alt="{380B8057-0B4B-4DD2-B139-BDECBE4A4D1B}.png" style="zoom: 50%;" />

### Back **Propagation**算法

**Backpropagation**（反向传播）是深度学习中用于训练神经网络的一种关键算法。

核心思想是通过计算损失函数相对于模型参数的梯度，来更新权重，使模型的预测误差逐步减少。![{1DE963D1-3025-4787-B05A-3798945F6C55}.png](https://github.com/Davereminisce/image/blob/master/%7B1DE963D1-3025-4787-B05A-3798945F6C55%7D.png?raw=true)

**前向传播（Forward Propagation）**：

- 先将输入数据通过神经网络层层传递，计算每个神经元的加权和并通过激活函数生成输出。最终在输出层生成预测值。

**损失函数（Loss Function）**：

- 预测结果与真实标签之间的差异通过损失函数进行衡量。常见的损失函数包括**均方误差**（MSE）用于回归任务，**交叉熵损失**用于分类任务。
- 损失函数定义了模型预测的误差大小，模型的目标是通过训练使这个误差最小化。

**反向传播（Backpropagation）**：

- 反向传播的任务是计算损失函数对每个权重的偏导数，也就是**梯度**，以便利用这些梯度来更新权重。
- 反向传播是通过链式法则（链规则，Chain Rule）来实现的，它会从输出层开始逐层向后计算每一层的梯度，直到输入层。

**梯度下降（Gradient Descent）**：

- 计算完每一层的梯度后，梯度下降法被用来更新每个参数。最常用的优化方法是**随机梯度下降（SGD）\**以及其变体（如\**Adam优化算法**）。

## 线性模型

###   Linear Model

<img src="https://github.com/Davereminisce/image/blob/master/%7BB5F08F52-E56F-4DC6-BBE6-BEF50BEF43A3%7D.png?raw=true" alt="{B5F08F52-E56F-4DC6-BBE6-BEF50BEF43A3}.png" style="zoom: 33%;" />

### Loss函数

Loss函数（损失函数）是用于衡量模型预测值与真实值之间差异的函数，它直接影响模型参数的优化过程。通过计算损失函数，模型能够知道它的预测有多准确或不准确，从而通过反向传播算法调整权重，逐步减少损失，使模型的预测越来越接近真实值。

#### 常见的Loss函数类型

不同任务通常需要选择不同的损失函数，以下是几类最常用的损失函数：

##### 均方误差（Mean Squared Error, MSE）

适用场景：回归任务。

公式： 

![{BA683C8C-DE08-4683-B1EA-C194C39DE484}.png](https://github.com/Davereminisce/image/blob/master/%7BBA683C8C-DE08-4683-B1EA-C194C39DE484%7D.png?raw=true)

##### 交叉熵损失（Cross-Entropy Loss）

适用场景：分类任务，特别是多分类和二分类任务

二分类公式

多分类公式

等等，有许多Loss函数

需要根据任务类型、数据特性、模型类型来选择

### eg（Linear Model）

![{04513495-7423-490F-B5B7-A4DE296D7363}.png](https://github.com/Davereminisce/image/blob/master/%7B04513495-7423-490F-B5B7-A4DE296D7363%7D.png?raw=true)

1、2行import是引入可视化图表库

### 可视化工具

模型训练时可以用**Visdom**进行**实时**可视化

![{B13D0245-DC21-4E3D-8505-CB001A0CB310}.png](https://github.com/Davereminisce/image/blob/master/%7BB13D0245-DC21-4E3D-8505-CB001A0CB310%7D.png?raw=true)

## 梯度下降算法

### 分治

将训练集分块，然后选择一个块再进行分块

坏处：效率低，可能错过更优点（如绿色曲线）<img src="https://raw.githubusercontent.com/Davereminisce/image/6423b034602248b725a0a2ceb4fd80bd99604d8f/%7B848B94E8-6A8E-474A-A6DC-69530EDC3EF2%7D.png" alt="img" style="zoom:50%;" />

### 全量梯度下降算法（Batch Gradient Descent）

为贪心算法（在局部区域最优）

<img src="https://raw.githubusercontent.com/Davereminisce/image/f684e7dfd4650dd7125d55057af5858e8aca93a2/%7BD259396D-67FD-4773-8252-16607451F305%7D.png" alt="img" style="zoom:50%;" />

#### 鞍点

它指的是在损失函数的局部区域中，梯度为零的点，但该点既不是局部最小值，也不是局部最大值。鞍点可能会对梯度下降过程产生负面影响，使得优化过程变得缓慢或难以收敛。

在一个多维函数中，其一部分方向的曲率是向下的（如局部最小值），而另一部分方向的曲率是向上的（如局部最大值）。换句话说，鞍点在某些维度上看起来像是最小值，而在其他维度上看起来像是最大值。

##### 鞍点对梯度下降的影响

算法可能停滞、收敛速度变慢

#### eg

优化上节中的eg用**梯度下降更新w**

<img src="https://raw.githubusercontent.com/Davereminisce/image/b74d9518f1bc44cdec0c04048692404d04ac8674/%7B1CB19269-27FF-498F-A374-18AE0C4CC81B%7D.png" alt="img" style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/928c8683ab3b1bd73c79afb7e1df15a8248380ec/%7B20A1C97F-9FAC-4B4F-9545-0217317C7A81%7D.png" alt="img" style="zoom:50%;" />

一般cost曲线会产生波动可用指数加权均值来使曲线变平滑

<img src="https://raw.githubusercontent.com/Davereminisce/image/f21c61ad001ecc164688875e5396f4eeb1615ec1/%7BF65BFF1E-EBE1-4619-9F3E-C2A41B40E574%7D.png" alt="img" style="zoom: 50%;" />

### 随机梯度下降（Stochastic Gradient Descent, SGD）

随机梯度下降引入了随机性，能够在较大规模数据集上高效进行优化。

<img src="https://raw.githubusercontent.com/Davereminisce/image/12987fcebc8bb6238438939ec5b4992ff44bec25/%7B3EAFE815-8B36-4813-B237-924A03BA70A5%7D.png" alt="img" style="zoom: 67%;" />

![img](https://raw.githubusercontent.com/Davereminisce/image/ced32e12443304081bbec9d645aba8726ae4cb91/%7BFDD460E6-BFC8-4AC0-9DA8-24B2EA878ED2%7D.png)

同时具有**随机性可以跨越鞍点**

不能对数据集进行并行运算，因为每对一个数据运算就更新一次w

#### eg

优化上节中的eg用**随机梯度下降更新w**

<img src="https://raw.githubusercontent.com/Davereminisce/image/b320742d33c27925f710d9ff25402f037df12738/%7BBA1647E4-A935-457C-8214-D5340D07151C%7D.png" alt="img" style="zoom:50%;" />

对每一个样本w都进行了更新

### 小批量梯度下降（Mini-Batch Gradient Descent）

用太多也直接省去Mini写作Batch

小批量梯度下降结合了全量梯度下降和随机梯度下降的优点。它在每次更新时使用一个小批量（batch）的样本来计算梯度，而不是全量数据或单个样本。

<img src="https://raw.githubusercontent.com/Davereminisce/image/aa2b1575b427097cb24a9ddd4a9d2442a80384c0/%7BE907664F-433D-4461-A657-E99587D52B1D%7D.png" alt="img" style="zoom: 67%;" />

## 反向传播

### 非线性激活函数（nonlinear activation function）

<img src="https://raw.githubusercontent.com/Davereminisce/image/de024b2b9cc3bbf5e470d653cac3b7fc1ec8b0f8/%7B29FBBD5E-BB83-4A1D-806C-54EE890AEE10%7D.png" alt="img" style="zoom:50%;" />

引入非线性激活函数（nonlinear activation function）

使模型具有非线性，从而能够处理复杂的数据模式。没有非线性激活函数，神经网络将会变得像线性模型一样，无法处理复杂的非线性问题。

常见的非线性激活函数：Sigmoid、Tanh（双曲正切函数）

### 反向传播

![img](https://raw.githubusercontent.com/Davereminisce/image/3e7bffb6cf9dfa0beef09010c92960204b08084b/%7B7D4D1913-4EE9-4BA4-B45A-60CE3C4075A8%7D.png)

#### eg1

#### <img src="https://raw.githubusercontent.com/Davereminisce/image/e1c57879fba6007f7b1b84881a7bd99c69ed266e/%7B4C9D730B-A0F4-4D6D-83CE-B0599D1E2732%7D.png" alt="img" style="zoom:50%;" />eg2![img](https://raw.githubusercontent.com/Davereminisce/image/035a77a5391159717789a09187eca9c416d6769d/%7B13D7B7F1-06F9-426D-A4AF-6302E594310E%7D.png)

b为偏置量

#### 偏置量

它通过引入一个额外的参数，帮助模型更好地拟合数据。

偏置量与权重w一样，也是通过反向传播算法和梯度下降算法来进行训练的。在每一次迭代中，偏置量的更新公式与权重相同

<img src="https://raw.githubusercontent.com/Davereminisce/image/f0ad8b6073990d81d116ac7fe514bbf2b7c0b156/%7B95E67B73-5D55-492D-9FC8-AB03843B914B%7D.png" alt="img" style="zoom: 67%;" />

### Tensor（张量）

是一种多维数组，可以表示标量、向量、矩阵及更高维的数据结构。

在**PyTorch**中，张量（Tensor）确实包含了两个重要的属性：**data** 和 **grad**。它们分别代表张量的数值和它在计算图中的梯度信息。

**data**表示张量的实际数值，即存储的原始数据。

**Grad** 表示张量的梯度，即在反向传播过程中计算出来的关于该张量的梯度值。

**requires_grad=True**表示张量需要在计算图中追踪梯度。

设置 `requires_grad=True` 的张量PyTorch 会开始为这个张量构建计算图，

当执行反向传播（调用 `backward()`）时，PyTorch 会根据损失函数（loss）计算每个张量对该损失的偏导数，并将结果存储在张量的 `grad` 属性中。

<img src="https://raw.githubusercontent.com/Davereminisce/image/51907e7966cca84433ed04c6e14147496dfe0f20/%7B50370DF2-6999-4270-BB82-377C1074B2AF%7D.png" alt="img" style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/6a5f30f6572fdbce538fc342c664cd88307a2ef9/%7BC5503666-95A6-4865-82DB-EB7D5B2278AA%7D.png" alt="img" style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/c2792eb5f60c1a1dd2be6822b22d8e7942041acd/%7B54894A18-5056-4826-AB78-6CA7BAE4F0D7%7D.png" alt="img" style="zoom: 67%;" />

**l.backward()**自动对链路上需要进行梯度运算的运算出结果并存到对应grad值，然后计算图释放。

**w.grad.data.zero_()**对w中的grad进行清零，方便下次循环，否则下次循环有误。

## 使用PyTorch实现线性模型

### 总体步骤

第一步 准备数据集

第二步 设计模型用来计算y上

第三步 构造计算损失的对象和用来优化的对象（using PyTorch API）

第四步 训练周期：前馈（算损失）、反馈（算梯度）、更新（更新权重）

#### 1.Prepare Dataset

x集合和y集合必须是矩阵，维度相同。（列数是维度）

<img src="https://raw.githubusercontent.com/Davereminisce/image/b0cedf4b5a6e2ec4d2c8960dea70c1a75747fbd9/%7BBABA97DD-2AE2-4926-8D78-62D51FFB5BA2%7D.png" alt="img" style="zoom:50%;" />

构造计算图（模型设计）

<img src="https://raw.githubusercontent.com/Davereminisce/image/d2252e46cb3487cb150c1eee70f48ebff3175f69/%7BC45AD7BA-AF82-4782-8A2F-805665F18D85%7D.png" alt="img" style="zoom:50%;" />

loss最后算出来是标量才能使用backward

#### 2.Design Model

![img](https://raw.githubusercontent.com/Davereminisce/image/b4d37c0cd2df9bd6cfa983f36f097e2767295edd/%7BBB148CD1-8F5B-4E44-98AE-9B077A284447%7D.png)

首先继承torch.nn.Module（也可用function但是这个需要自己设置backward）

 _ _init _ _():构造函数，初始化对象

forward（）：前馈过程需要执行的计算，名字就只能叫forward

backward（）：不用写，因为Module会自动帮你生成对应backward

##### torch.nn.Linear（，）

torch.nn.Linear（，）：构造对象，对象包含权重和偏置。torch.nn.Linear（1，1）专指构造出y=x+1（linear中第一个数字是维度数，第二个是输出维度）

<img src="https://raw.githubusercontent.com/Davereminisce/image/95f08f9bd1895dafc2f8b1fd57da42dcb2d0015e/%7B1743A34C-1FA2-4C4F-A3D5-2F42CB4E059F%7D.png" alt="img" style="zoom: 50%;" />

#### 3.Construct Loss and Optimizer

<img src="https://raw.githubusercontent.com/Davereminisce/image/2f735801aca57dbd65d5736eb33c1ff0f7932714/%7B9A308F57-4A95-476B-8345-09E2405419D0%7D.png" alt="img" style="zoom:50%;" />

##### criterion（损失函数）

直接调用Module中的MSE函数，这里false表示不求1/n

##### optimizer（优化器）

model.parameters（）会检查model中所有成员，找出需要计算权重的成员，有权重就都加到训练上。

lr（学习率）：即设置学习率

#### 4.Training Cycle

![img](https://raw.githubusercontent.com/Davereminisce/image/4297773cccaa2fd5df78d3d02126cfb7021a988a/%7B0CF1914D-7E6C-4074-9B5B-6744AE1CDD36%7D.png)

第一步算出y上

第二步算出损失，然后打印这里loss本来是计算图，但是打印时调用_ _ str_ _()不会产生计算图

第三步把所有权重归零

第四步进行反向传播

第五步用step函数进行更新，step能根据所有参数的梯度、学习率自动进行更新

#### 5.打印输出

![img](https://raw.githubusercontent.com/Davereminisce/image/a7ba02c00c3aadc1bda1a7425b669ff5b30f183f/%7B7AD43430-B373-4D7E-9F84-0E6313CFDD1A%7D.png)

这里model.linear.weight.item()打印输出w的值，model.linear.weight()输出的是矩阵形式

### 整体代码

![img](https://raw.githubusercontent.com/Davereminisce/image/038c95542bcbbf99dbc79496f63f21eed802187f/%7BF413B46F-CF17-4D20-A081-8A7093D84B2C%7D.png)

## 逻辑斯蒂回归（分类）

分类中输出的是属于每个分类的概率，然后找出最大的

### MNIST数据集

```python
import torchvision

# 定义数据的预处理步骤，如将图像转换为张量
transform = transforms.Compose([transforms.ToTensor()])

# 下载并加载 MNIST 数据集
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)
```

### 逻辑斯蒂回归

模型输出是某个类别的概率。

用函数使输出的实数集映射到[0,1]

<img src="https://raw.githubusercontent.com/Davereminisce/image/57646cebdaeb14250317fac37351103c84faea3d/%7B96F842E4-DC8F-4A4F-8CAE-18CDB02ACB45%7D.png" alt="img" style="zoom:50%;" />

#### sigmoid functions（激活函数）

<img src="https://raw.githubusercontent.com/Davereminisce/image/d2fb26089c7677fe0c18993e5566477edbd6dff3/%7BC061E30D-C225-43FA-BFFA-D98FF64E9B62%7D.png" alt="img" style="zoom:50%;" />

一般sigmoid可以直接指向Logistic Function

#### Model

计算图改变

<img src="https://raw.githubusercontent.com/Davereminisce/image/5eac6d9c22e03f659770109027ca9c475da4591e/%7BA71B2C09-ACB0-4E4E-AD77-94A0370D109B%7D.png" alt="img" style="zoom:50%;" />

loss Function改变

这里用二分类交叉商

![img](https://raw.githubusercontent.com/Davereminisce/image/c16913f5965b3e5ff9169e2c1a898aa7c2a668a6/%7BC8FF471A-E13C-4B65-B3B3-47FE8E6F2D21%7D.png)

<img src="https://raw.githubusercontent.com/Davereminisce/image/f78512ca2d4922e0b8fa7e93d2d425d892bfe2cd/%7B74C55F79-E204-4D8E-9285-41B87593C48F%7D.png" alt="img" style="zoom:50%;" />

import torch.nn.functional as F引入F，F里面有sigmoid等sigmoid function

init不变，forward改变

#### criterion（损失函数）

<img src="https://raw.githubusercontent.com/Davereminisce/image/4bfb4ed67aa94b907dbd6cb4752f97fc3ddacc6f/%7B85ECC77B-0977-4E61-9328-B84749D7BC2D%7D.png" alt="img" style="zoom:50%;" />

原本MSE变为BCE就行了

#### 整体代码

<img src="https://raw.githubusercontent.com/Davereminisce/image/facb127ef0ad1a5a10d16ba9ba66173c32b4c1af/%7BF1E7A4AC-6C9B-49D6-9D14-3B87ABE4FE7D%7D.png" alt="img" style="zoom:67%;" />

## 处理多维特征的输入

### 多维处理的Model

<img src="https://raw.githubusercontent.com/Davereminisce/image/1f5f932160215a40edf55c52694baf5d40a3c20b/%7B04FC9A25-AD25-492A-84D5-D3BDB5042CB5%7D.png" alt="img" style="zoom:50%;" />这里sigmoid这类函数的也称为激活函数

<img src="https://raw.githubusercontent.com/Davereminisce/image/8d36283dedfd54b6709653730cf3088f309a01e6/%7B5DEEC0DB-67B2-4C70-8C7E-48851289C517%7D.png" alt="img" style="zoom: 67%;" />

#### 非线性变换设计

<img src="https://raw.githubusercontent.com/Davereminisce/image/5d3b5426ced87d869785e212872e3f940b9be752/%7B0926636B-65BE-438C-AD71-910E9DA1D394%7D.png" alt="img" style="zoom: 50%;" />

这里Linear（8，6）指8维降至6维

神经网络构造

<img src="https://raw.githubusercontent.com/Davereminisce/image/77ca568822ed056db029404e5319100543545e7d/%7BC36C236A-7A9F-4891-B1AB-2DB39579DB2D%7D.png" alt="img" style="zoom: 50%;" />

#### 算法设计

##### Prepare Dataset

<img src="https://raw.githubusercontent.com/Davereminisce/image/4910950a41c53422e365ce317982c61339e865ef/%7B5C91C46C-2EE4-4101-9713-15B79CB74CF9%7D.png" alt="img" style="zoom: 50%;" />

：-1指最后一列不读

[-1]表示读出来的是最后一行矩阵

##### Define Model

<img src="https://raw.githubusercontent.com/Davereminisce/image/b4767ecc05243cbaca031d1774c884bb3e9fca5c/%7B9462599C-AE20-4EC4-8C35-421B73EF17A2%7D.png" alt="img" style="zoom:50%;" />

##### Construct Loss and Optimizer

没啥变化

<img src="https://raw.githubusercontent.com/Davereminisce/image/44441b1bd8b9d540658be86600a3feb303b7d5be/%7B4BDB8C2A-DB3D-45F2-8276-A58DBD321970%7D.png" alt="img" style="zoom:50%;" />

##### Training Cycle

也没啥变化

<img src="https://raw.githubusercontent.com/Davereminisce/image/e47ae56b0f0fa9945031935cbd73b957ee030abb/%7B968A9A3C-55D4-43B5-A274-138A57A0D9ED%7D.png" alt="img" style="zoom:50%;" />

## 加载数据库

### Mini-Batch运用

重点在于构造数据集，然后循环变为嵌套

#### 嵌套循环

<img src="https://raw.githubusercontent.com/Davereminisce/image/eebd14d866f9c8d30974b5b2c180acb44b80a67a/%7BB3FECD08-F890-4B2D-BF50-2DBC501B71FC%7D.png" alt="img" style="zoom:50%;" />

#### Dataset&DataLoader

<img src="https://raw.githubusercontent.com/Davereminisce/image/1031df2b86d2e61d899075970c894a19b97c0253/%7B5A864DA6-9FBB-46AB-8F5D-FC7B7B0F1741%7D.png" alt="img" style="zoom:50%;" />

shuffle是打乱原数据

<img src="https://raw.githubusercontent.com/Davereminisce/image/a8b1a41b098523eb1e36c0e5bfffe70a06b8700f/%7B265E59CA-AB34-40F2-A8C0-4A737B34F8FE%7D.png" alt="img" style="zoom:50%;" />

mun workers指多线程数量

#### Windows中运行Cycle问题情况处理

在Windows中多进程库不一样，需要更改，将迭代代码封装一下就行

<img src="https://raw.githubusercontent.com/Davereminisce/image/1662b19a96d59ee9626ba66665dbf107cd3388c7/%7B383F6A53-3C6C-449F-994C-8C4407EC534A%7D.png" alt="img" style="zoom:50%;" />

更改之后

<img src="https://raw.githubusercontent.com/Davereminisce/image/91245ec6645b8f1684ff7f330af97fedefad6ba2/%7B490486C8-1EF9-4C25-BAD1-056563C24769%7D.png" alt="img" style="zoom:50%;" />

#### eg（Mini-Batch运用实例）

##### Dataset&DataLoader

![img](https://raw.githubusercontent.com/Davereminisce/image/a8838f87526de82af4eb69af9e24636cc7f8a833/%7B042061B2-721E-44BC-A5AF-44836AA0496B%7D.png)

##### Train Cycle

<img src="https://raw.githubusercontent.com/Davereminisce/image/28b7965506283751dce3df884be571e690b958af/%7B951D8219-99B5-4315-A9A1-F23BF6BA0621%7D.png" alt="img" style="zoom:50%;" />

##### 整体

<img src="https://raw.githubusercontent.com/Davereminisce/image/1c47eea54234b747378623f0eef0edcd1e4960e4/%7B069FA955-C251-4919-9A87-581540815CE6%7D.png" alt="img" style="zoom: 67%;" />

#### eg（MINST数据集直接引入并使用Mini-Batch）

![img](https://raw.githubusercontent.com/Davereminisce/image/21a270b468737184029e44fa672187820de09319/%7BA559D66C-C942-43E5-9EA6-D773507903DD%7D.png)

这里漏掉了epoch，别忘了

#### Q:是否需要构建DiabestesDataset类

构建 DiabetesDataset 类的主要目的是为了创建一个自定义数据集。

已经构建了数据集可以不用构建一个DiabestesDataset类，构建DiabestesDataset类是为了创建数据集。

比如如果数据存储在 TXT 文档中，为了对这些数据进行特定的处理或管理，就需要构建DiabestesDataset类

## 多分类问题

希望输出之间也有联系，都>=0；和为1。

<img src="https://raw.githubusercontent.com/Davereminisce/image/453040080ff6575fcd8903dc8ca4465958de2c91/%7B9C0B5F8A-3940-48D8-8772-15B11C56E529%7D.png" alt="img" style="zoom:50%;" />

### Softmax Layer（激活函数）

常用于多分类任务的激活函数，通常用于神经网络的最后一层。

它将网络的输出转换为概率分布，使得每个类别的预测值在 0 到 1 之间，并且所有类别的概率和为 1。

<img src="https://raw.githubusercontent.com/Davereminisce/image/e6a3ab5bf562197286fa398579c392440fc76b46/%7BD82F5497-DEA7-44E9-B0ED-671C20169F71%7D.png" alt="img" style="zoom: 80%;" />

#### loss函数改变

#### <img src="https://raw.githubusercontent.com/Davereminisce/image/cb4e81847a8cb01fc046c1c39676458b662845b9/%7BFA59D90D-B0AA-44F5-B5AF-FF4EC3D0C561%7D.png" alt="img" style="zoom:50%;" />cycle改变

<img src="https://raw.githubusercontent.com/Davereminisce/image/c884d0e1c5e42b340ef7c6a79dcd862653219255/%7B0A000954-2601-480A-A177-6D39973530CF%7D.png" alt="img" style="zoom: 50%;" />

这里np.array([1,0,0])指第一类概率为1，第二类为0，第三类为0

y = np.array([[1, 0, 0], [0, 1, 0]])表示两个数据第一个为第一类，第二个为第二类

<img src="https://raw.githubusercontent.com/Davereminisce/image/1265b908cc51b74100ae58ae2f62e77c4cfb5970/%7B8FFF8447-E039-4BE0-BF88-B4C9BFDEB174%7D.png" alt="img" style="zoom:50%;" />

这里torch.LongTensor([0])：指第0类

多个数据并行

<img src="https://raw.githubusercontent.com/Davereminisce/image/88acc4c419f0c09c488054441f4b9f723a992abf/%7B685DEFC0-9540-4840-83BA-F0525E691C10%7D.png" alt="img" style="zoom:50%;" />

#### np.array与torch.LongTensor区别

y = np.array([[1, 0, 0], [0, 1, 0]])某种程度上等价于y=torch.LongTensor([0，1])

表示类别上是相同的

`y = np.array([[1, 0, 0], [0, 1, 0]])` 表示两个样本的 one-hot 编码：

第一个样本属于第 0 类。

第二个样本属于第 1 类。

`y = torch.LongTensor([0, 1])` 表示一个长整型张量，其中：

第一个元素是 `0`，表示第一个样本属于第 0 类。

第二个元素是 `1`，表示第二个样本属于第 1 类。

区别在于

`np.array([[1, 0, 0], [0, 1, 0]])` 是 one-hot 编码，提供了类别的完整信息。

`torch.LongTensor([0, 1])` 是类标签的简单表示，不提供额外的编码信息。

### eg（图像训练）

#### 1.Prepare Dataset

![img](https://raw.githubusercontent.com/Davereminisce/image/1a31cc148f18888cb5d5173a5251f77ced3e6d8b/%7BFA36659E-51A0-44F4-8D34-AF79BD62D80B%7D.png)

transforms.ToTensor():指将图像张量进行转化，这里1 *28 *28中，1是通道，28是宽，28是高

transforms.Normalize指归一化0.1307是均值，0.3081是标准差，这个值是MNIST集计算得出的

#### 2.Design Model

<img src="https://raw.githubusercontent.com/Davereminisce/image/10d6a145d8bfa14689bf3aa01a55fc1d03fe4f6e/%7B2594A48E-0BB3-460F-BBD3-C07F9780CC4F%7D.png" alt="img" style="zoom:50%;" />

##### x = x.view(, )

`x = x.view(-1, 784)` 是 PyTorch 中用于调整张量形状的一个方法。具体来说，这行代码的含义是将张量 x 的形状更改为一个二维张量，其中每行有 784 列，而行数会根据原始数据的大小自动计算。

-1：这个参数的作用是自动计算该维度的大小，以确保总元素数保持不变。PyTorch 会根据 x 的总元素数和指定的其他维度（这里是 784）来确定行数。

784：这是指定的每行的列数。在处理 MNIST 数据集时，每张图像的大小是 28x28 像素，所以展平后每张图像会有 784（28 * 28）个像素值。

#### 3.Construct Loss and Optimizer

<img src="https://raw.githubusercontent.com/Davereminisce/image/415e5477032e66240277a98ed9fb5cf6f2f943c6/%7B8FC10FEC-91B7-49CD-ABB6-1654AFFB1684%7D.png" alt="img" style="zoom:50%;" />

#### 4.Train and Test

##### train

![](https://raw.githubusercontent.com/Davereminisce/image/master/%7B4370F984-E8EE-4195-B10A-E7DD60221E52%7D.png)

这里if语句表示每300次迭代才输出loss

##### test

<img src="https://raw.githubusercontent.com/Davereminisce/image/c38a26c929af0ca78968bc37a9d19c6054f71f31/%7BC9C45DE7-B533-43DF-816D-72B7FFB5A048%7D.png" alt="img" style="zoom:50%;" />

torch.no.grad()：表示不用回退

_, predicted=torch.max(outputs.data, dim=1)：这个函数返回 _->最大值，predicted->最大值下标，dim=1：指定沿着第 1 维（列）进行操作这样每一行代表一个样本的输出。

labels.size(0)：用于获取张量 labels 的第一个维度的大小，通常表示样本的数量或批次的大小。

##### 迭代

<img src="https://raw.githubusercontent.com/Davereminisce/image/cfa25548ae9215a37103f4b9d35e478f6a034635/%7B7AE8F96B-3C6E-4293-92F7-E31C0B9C28A7%7D.png" alt="img" style="zoom:50%;" />

## 卷积神经网络CNN（基础）

### 思路

<img src="https://raw.githubusercontent.com/Davereminisce/image/ced32bd3fd9366d8df1f95c4717ef128727818fb/%7B4FBF2600-C4C3-460B-B2A4-F8304CB4EFA7%7D.png" alt="img"  />

### 卷积

#### 单通道卷积

![img](https://raw.githubusercontent.com/Davereminisce/image/870457e4c103eb41de309e2af1d97b5c5fd81ad9/%7BC97304AB-6FD4-4421-8AE3-7D8BB4842DB5%7D.png)

找每个3*3矩阵与K数乘

#### 多通道卷积

##### 一个卷积核

<img src="https://raw.githubusercontent.com/Davereminisce/image/19b972a9026fb776bb451f19ec44ec8952364edd/%7B2A089CC2-453F-4B59-9D29-60905B1892F7%7D.png" alt="img" style="zoom:50%;" />

通道数和卷积核数量相同

##### 多个卷积核

<img src="https://raw.githubusercontent.com/Davereminisce/image/a73a169b73af1e0f72c67dd98c78b2db8bb27cf1/%7B4EFCE8D9-A908-40A3-B6C2-669E53BD8E7E%7D.png" alt="img" style="zoom: 50%;" />

![img](https://raw.githubusercontent.com/Davereminisce/image/18639b358333e47acdf17970c7f57bc517deab14/%7B3A7EE3C2-DDBB-46EC-BA34-792649D0F7E3%7D.png)

将m个卷积核合并形成四维卷积核

#### eg

<img src="https://raw.githubusercontent.com/Davereminisce/image/3fe6cb7f960f06cd4468845c1a680d5936d10f23/%7B39C8E253-9927-4588-A6E6-D21A484CD309%7D.png" alt="img" style="zoom:67%;" />

输出为

torch.Size([1, 5, 100, 100])

torch.Size([1, 10, 98, 98])

torch.Size([10, 5, 3, 3])

#### padding&stride

##### padding

在Input外加0，padding=x相当于加x层0。

<img src="https://raw.githubusercontent.com/Davereminisce/image/7ff22afb6ce1467df1509028906f7ba20789c5cb/%7B2839FCE0-520C-4601-9BFC-E82A00EB5138%7D.png" alt="img" style="zoom:50%;" />

padding=1

相应代码实现

<img src="https://raw.githubusercontent.com/Davereminisce/image/c74937029631fc8d3d552a76b0bd8fdb475775cc/%7B53934FC5-DC6B-44B4-85E5-87A360E4C6CB%7D.png" alt="img" style="zoom: 50%;" />

##### stride

每一块计算（红色框那个）移动stride个

<img src="https://raw.githubusercontent.com/Davereminisce/image/d42cf2ad3e88c157d48f9c4e762105e6a426f363/%7B7DC0F255-4578-4F80-B05F-A636ACF2FA3B%7D.png" alt="img" style="zoom:50%;" />

stride=2

相应代码与padding差不多，只是padding=1改为stride=2

### 池化

#### MaxPooling

一种常见的池化操作，用于减少特征图的空间尺寸，从而降低计算复杂性和防止过拟合。

Max Pooling 在特定的窗口（如 2×22 \times 22×2 或 3×33 \times 33×3）上滑动，并从该窗口内的值中取**最大值**。

<img src="https://raw.githubusercontent.com/Davereminisce/image/51ae2b74f551b7f9c20581466779d0c064b971da/%7B197C3333-72E9-4433-AEAC-C2BF278B35F4%7D.png" alt="img" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/ea6c81a792de45b67799ac65527209dfaaaef6c9/%7B43DC5005-803A-429C-829F-7BC08C07B506%7D.png" alt="img" style="zoom: 50%;" />

此时默认步长（stride）等于2

### 卷积神经网络

#### Model



<img src="https://raw.githubusercontent.com/Davereminisce/image/864a9263c5840b6b26f66c6398f8298874bb6fb4/%7BC0D13F1A-AE34-4687-9316-CA0EA1D887DC%7D.png" alt="img" style="zoom:50%;" /><img src="https://raw.githubusercontent.com/Davereminisce/image/91ca11637fb4c8a47a8ec6fcfa81286ceb8bf24a/%7B1DC184BA-2084-4697-8D8E-C1074E04F23F%7D.png" alt="img" style="zoom:80%;" />

##### 代码实现

<img src="https://raw.githubusercontent.com/Davereminisce/image/61afe43c04af052b2e9b712645a22707189ca6cb/%7B1BC103C5-3EE9-4C1D-BA60-82A0A69110A3%7D.png" alt="img" style="zoom: 50%;" />

**ReLU** 是一种高效的激活函数，广泛用于深度学习模型中，能够有效捕捉非线性特征。

##### 为什么要激活函数

激活函数的最主要目的是为网络引入**非线性特性**。如果没有激活函数（例如只使用线性变换），无论网络的层数多深，整个网络实际上仍是一个线性模型，无法处理复杂的非线性关系。

激活函数（如 ReLU）能够输出大量的零值，使得神经网络具有稀疏激活的特性。这种稀疏性能够提高计算效率并减少过拟合。

激活函数在神经网络中至关重要，主要作用有：

1.引入非线性，使网络能够处理复杂的任务。

2.保持特征多样性，帮助网络学习更丰富的表示。

3.解决梯度问题，确保网络参数可以通过反向传播进行优化。

4.启发式学习数据中的特征。

5.提供稀疏激活的特性，提升计算效率。

### 如何在GPU上运行Model

<img src="https://raw.githubusercontent.com/Davereminisce/image/b1715b93f4ffd0e0aaec488a8841660dfe784e9a/%7B860045EC-9215-47C4-8182-DAAADB48ED3E%7D.png" alt="img" style="zoom:50%;" />

```python
device = torch.device("cuda：0" if torch.cuda.is_available() else "cpu")
model.to(device)
```

<img src="https://raw.githubusercontent.com/Davereminisce/image/a2d1881e7637b0d8da5797d362195a4eaeb53d40/%7B80FE19CA-F39A-4E52-9A2B-EE9DAB480D51%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/330b377355c5f36c8a7c026d5eadf369b214050c/%7BC652F374-C67A-4FAE-BE9F-DC84900DFFB4%7D.png" alt="img" style="zoom:50%;" />

```python
inputs, target = inputs.to(device), target.to(device)
```

训练和测试都需要添加

## 卷积神经网络（高级）

### GoogleNet

<img src="https://raw.githubusercontent.com/Davereminisce/image/d16ec54de2821470043f1cd4054aeacaa817365f/%7B3D54640C-B48E-4CDD-ADFE-BE88611FBAD7%7D.png" alt="img" style="zoom: 50%;" />

一个结构重复的块称为Inception

### Inception

<img src="https://raw.githubusercontent.com/Davereminisce/image/f7a4c5c16020716397719adf17ec792a1375b91d/%7B12B2783D-AB50-4FB3-A4D8-0184BF2D99E5%7D.png" alt="img" style="zoom:50%;" />

#### 1*1卷积

1*1卷积可以跨越不同通道相同位 置的元素值使信息融合

<img src="https://raw.githubusercontent.com/Davereminisce/image/b22a60ecff299e595d1f0d49216700d32333e908/%7B54DCB690-2B53-4A5C-A773-79AD612DFD28%7D.png" alt="img" style="zoom:50%;" />

1*1卷积可以减少运算数量

<img src="https://raw.githubusercontent.com/Davereminisce/image/0880a38d8c1e83d563af21b03b77c59b9e4d0422/%7B34B20DA1-7AC2-476E-B9DF-BE6E6CC47FB5%7D.png" alt="img" style="zoom:50%;" />

#### Inception Module

<img src="https://raw.githubusercontent.com/Davereminisce/image/2981fe886560139fe11c349a27a9f92d753e8eeb/%7BD9734CF5-5779-4EA1-9A63-A5909A70E64A%7D.png" alt="img" style="zoom:50%;" />

第四个模块有三行操作（图上显示不全）

<img src="https://raw.githubusercontent.com/Davereminisce/image/f1e5dfb0ea3402364fd25af213054f8defbcecb8/%7B659C9EA5-8864-40C7-B512-DD36EF17721B%7D.png" alt="img" style="zoom:50%;" />

这里dim=1：指按照c（通道）来合并

**最后输出通道数是88**

#### 完整代码&eg

<img src="https://raw.githubusercontent.com/Davereminisce/image/0c1d2cdc2b7102a29abd25d57c262b74d516e2f7/%7B65BFF9AC-DA36-4FD2-9488-8B12F2F02BB2%7D.png" alt="img" style="zoom:50%;" />

in_size指batch_size批次大小即输入数据的样本数量。

这里1408是由输入数据集和Model共同得出，可在编程过程中先算出值再继续填充代码

### ResNet（深度残差网络）

#### 深度残差学习（Deep Residual Learning）

用于解决深度神经网络随着层数增加而出现的**梯度消失**和**退化问题**。该结构被称为**残差网络（Residual Network，ResNet）**。

![img](https://raw.githubusercontent.com/Davereminisce/image/d51a040dd6e813d6bb79c28d972ab73f4994a13a/%7B35D21217-A098-4F16-A15D-15680CAD168A%7D.png)

<img src="https://raw.githubusercontent.com/Davereminisce/image/2574cfaa0b18ec0cd26bf171030b177d79e32f90/%7B602DD09C-0DA2-4C59-9773-FB3EF0B72DEF%7D.png" alt="img" style="zoom:50%;" />

##### 残差块

一个典型的残差块包含以下部分：

两个卷积层，每个卷积层后面跟着批量归一化（Batch Normalization）和 ReLU 激活函数。

输入x直接通过跳跃连接与残差函数 F(x) 的输出相加。

##### 代码实现

<img src="https://raw.githubusercontent.com/Davereminisce/image/40a367f36462f5ecd83b09d7cef404ef3b6bc9c5/%7B51705662-A73F-46A7-93E5-895DEC7CA8A4%7D.png" alt="img" style="zoom:50%;" />

#### 深度残差网络代码实现

<img src="https://raw.githubusercontent.com/Davereminisce/image/ccffc3b0bbcfa217d38b2ce4657f324c466e2348/%7B55045ACB-5FAA-4A4A-A2D3-9BF5C25DDCB9%7D.png" alt="img" style="zoom: 33%;" />

## 循环神经网络RNN（基础）

<img src="https://raw.githubusercontent.com/Davereminisce/image/0f2dcfb5c89a12cf7d8f1d1c87098f4b1c40e5ab/%7B59D787FB-FB53-41D2-B10C-EAFCDCC81003%7D.png" alt="img" style="zoom:50%;" />

RNN Cell只有一个，即w只有一套

### RNN计算过程

<img src="https://raw.githubusercontent.com/Davereminisce/image/060be426395917033d75d47592452eb9323db2e2/%7B3D01C0CE-52B8-4FD9-81A0-E946CA081997%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/cbe9511173474ff845d20c64e2d7cb936df3156e/%7B3F2ED11C-EF39-4446-A8B0-15AD470703EA%7D.png" alt="img" style="zoom:50%;" />

#### RNNCell

<img src="https://raw.githubusercontent.com/Davereminisce/image/0e47efee6187eb5d0d59697a7a9d44f121561a08/%7BE883F0D4-D195-43E2-9950-5BEC9A98971C%7D.png" alt="img" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/f077981e09ef703b6ac2ec26464c9c57778741bd/%7B74BF52F1-5070-4DD4-AC2E-821F7BCF2E95%7D.png" alt="img" style="zoom: 50%;" />

##### seqLen

**序列长度**：表示序列数据中的元素个数或时间步数，如句子的单词数或时间序列的时间步数。它主要描述序列的长度。

**维度**：表示数据在张量中的轴大小，可以描述数据的不同方向，如批次大小、特征维度等。它更普遍地描述数据形状中的不同维度。

序列长度可能在某些情况下表现为数据的一个维度，但它并不等同于一般意义上的维度，维度可以有更多的含义。

##### RNNCell代码实现

<img src="https://raw.githubusercontent.com/Davereminisce/image/6ee35e37a7ccdf444c88dfc74069ffaf4f35da8c/%7B6F7B32C4-A1D9-4F0E-8CF2-9802ABABAD65%7D.png" alt="img"  />

#### RNN

相比于RNNCell不用写循环，并可直接算多层递归

![img](https://raw.githubusercontent.com/Davereminisce/image/46cf3df903de7b1767630af1be062d00f18a8704/%7B67611640-88A6-411C-A00E-5B28EC3B2E0A%7D.png)

##### num_layers

**num_layers**指模型中堆叠的隐藏层的数量，即网络中的层数。

这里num_layers=1

![img](https://raw.githubusercontent.com/Davereminisce/image/fa5b7e6918da5e24726effc5ff6ed2f9377e56e4/%7B840992BF-F45C-4F6F-905E-BCB226372B1F%7D.png)

这里num_layers=3

同层的RNN Cell才是同一个

<img src="https://raw.githubusercontent.com/Davereminisce/image/057ce329abe5f8f3e9ba6e99fa418f8eef08b014/%7BACB0E479-C5BF-431B-93B7-ED50950D90DA%7D.png" alt="img" style="zoom:50%;" />

##### RNN代码实现

shape of output（seqSize,batchSize,hiddenSize）

shape of hidden（numLayers,batchSize,hiddenSize）

![img](https://raw.githubusercontent.com/Davereminisce/image/51ebfb4e8d3ce201df64480dea44870538890a83/%7B5A7C6DCD-02DB-4523-BA8D-8A7AFA002189%7D.png)

### eg1使用RNNCell（字符串转换）

#### 字符数据处理

<img src="https://raw.githubusercontent.com/Davereminisce/image/6db1bcd2e8d77e219c9af273422e61461a28c66e/%7B1543D6A7-ACB7-4B9C-9236-92717F5F12E3%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/7c4e69bee7ece290be67e730ec0bed769f3dd6ab/%7BAD51DE94-ED75-495F-AF0E-B2A0B88124D0%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/f6bc56c750a811a378cd04717a42ff979e0d9798/%7B7EE88DF0-8F1D-44BD-B926-1B91370D652F%7D.png" alt="img" style="zoom:50%;" />

#### 代码实现

##### prepare data

<img src="https://raw.githubusercontent.com/Davereminisce/image/aa8249a0d4805da6253210f9acb94e2b9c644ed2/%7B950266BF-84F9-43FB-8442-63FC8AD442E7%7D.png" alt="img" style="zoom:33%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/bf86c8d01fa3eec77f9ced9a0883effd501b3c10/%7B5954634D-AC1B-450C-9AB2-147EB5DE46D9%7D.png" alt="img" style="zoom:50%;" />

##### Design Model

<img src="https://raw.githubusercontent.com/Davereminisce/image/040b6beb75cfe2beb5807631a86adc485ad1245c/%7BB7AA0CBF-7F77-42DE-AB66-13925D65BF89%7D.png" alt="img" style="zoom:50%;" />

##### Loss Optimizer

<img src="https://raw.githubusercontent.com/Davereminisce/image/d4e2fe5ec682ae9a123a0e2a7b4e52d97b2dcbd1/%7B6BC2C14C-E7FD-4E48-910D-61B45DE33C7C%7D.png" alt="img" style="zoom: 50%;" />

##### Training Cycle

<img src="https://raw.githubusercontent.com/Davereminisce/image/53579434e676b6816fd085fd5a8b67b1274c146c/%7B6E23CBCE-698D-4B77-8F89-AD15F688BF1D%7D.png" alt="img" style="zoom:50%;" />

### eg2使用RNNCell

<img src="https://raw.githubusercontent.com/Davereminisce/image/7ad8bfb5049c3ea9efbf5053531987dc271399b0/%7BB3E0A004-780C-4F98-834B-6432AF3059C1%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/d4178a36346041e26b33956e0c47af4d0dd209a9/%7B22789548-8E15-4017-8A67-BCABBC95CD2A%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/a16b5da7326e51ad318e2464892742ed64dd0836/%7B48FC78D3-70A4-4D04-BAB9-85F976540023%7D.png" alt="img" style="zoom:50%;" />

### Embedding

相比于one-hot数据降维

Embed用于将离散的输入（如单词、字符或类别）转换为连续的向量表示。

嵌入（Embedding）层能够将高维的稀疏数据（如词语、类别等）映射到一个低维的连续向量空间

<img src="https://raw.githubusercontent.com/Davereminisce/image/d6320357ccacbbdc80ce0c18db5c2037aeaa483c/%7B47FA3146-FCFD-45DD-B819-7F4348B6F7B6%7D.png" alt="img" style="zoom: 67%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/1e7d64b739480173a234f04ef344f2c570493b33/%7B59D0234D-6827-4319-90C8-8AFF4F19C032%7D.png" alt="img" style="zoom:50%;" />

#### eg 将Embedding加入



<img src="https://raw.githubusercontent.com/Davereminisce/image/48f1fc2849bb7b31ddcf7bc6f881bf2beaf84e39/%7BE14EC843-F405-4F1C-A8D0-2B8EEC505A52%7D.png" alt="img" style="zoom: 33%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/67bc3901af8f0e5c2624c60b10a50ff3c910dd26/%7BCEB66731-E5EA-45EE-8464-C2FEC0E3081E%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/a97b7995600ed2784a92cb714594afe770072dec/%7BB4DC84E3-2DCF-4E3D-833A-D2874B5A8138%7D.png" alt="img" style="zoom: 50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/8acd8d54f848114e1784dce19dd06ddae2d2d01a/%7B670D066C-63B5-4E10-921D-BED02E57F45D%7D.png" alt="img" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/Davereminisce/image/d0b4e20e1005d613f1b60afb9cccf1dc5f53ba8d/%7B691AF170-EF8D-44B5-A375-6E1DE4A4248D%7D.png" alt="img" style="zoom:50%;" />

## 循环神经网络（高级）

这节是循环神经网络的运用，便在之后在学习吧。。。

2024/10/16
